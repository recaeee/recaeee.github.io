---
layout:     post
title:      "浅谈FSR1.0 in Unity URP"
subtitle:   ""
date:       2023-06-16 12:00:00
author:     "recaeee"
header-img: "img/in-post/default-bg.jpg"
tags:
    - 浅谈系列
    - 超分辨率
mathjax: true
---

# 浅谈FSR1.0 in Unity URP



![20230616181031](https://raw.githubusercontent.com/recaeee/PicGo/main/recaeee/PicGo20230616181031.png)



---

#### 0 写在前面

环境：
Unity：2021.3.12.f1
URP：Universal RP 12.1.7

#### 1 URP中FSR使用限制 FSR Restriction in URP

由于FSR使用到了TextureGather，因此设备支持的**Shader Model必须大于4.5**（从OpenGL ES 3.1开始）。如果为设备开启了FSR，但是设备支持的Shader Model小于4.5，会Fallback到Automatic filter模式上采样。

当**FXAA和FSR共同开启**时，**FXAA会在一次单独的Blit的Draw Call中完成**（不开FSR时FXAA可以直接在Uber的Draw Call中和其他后效一起合并完成），然后再执行FSR上采样。分开的原因是FXAA的输入输出RT必须尺寸相同，而开启FSR实现上采样时输入RT尺寸比输出RT小。


#### 2 URP中所有上采样模式 All UpScaling Modes in URP

1. **Auto**:在Auto模式中，URP会尝试根据当前情况选择（设置的RenderScale）最优的上采样采样器（其结果只有3种，Point、Linear、None）。

如果RenderScale的设置值导致原始RT的一个像素正好对应上采样后RT的整数个像素，即将原始RT上采样至其尺寸的整数倍，则使用Point Filter进行采样，以RenderingScale=0.5为例（即需要上采样到原RT的2倍），此时上采样得到新RT的每个像素时不存在新像素覆盖到2个以上原RT像素的情况，因此可以直接使用Point Filter对单个像素进行采样。

如下图所示，RenderScale设置为0.5，即将渲染得到的RT的长宽各扩大一倍进行上采样，此时原始RT上1个像素正好贡献给最终RT的4个像素做上采样，并且原始RT上各像素之间不会相互覆盖到其贡献的像素，这时候使用Point Filter即可。



![20230613173508](https://raw.githubusercontent.com/recaeee/PicGo/main/recaeee/PicGo20230613173508.png)



如下图所示，RenderScale设置为0.75，以实际渲染得到6x6的RT为例，最终会上采样到8x8的RT，此时多个原始RT的像素都会对最终RT的1个像素做出贡献，因此需要使用线性采样器。



![20230613173913](https://raw.githubusercontent.com/recaeee/PicGo/main/recaeee/PicGo20230613173913.png)



2. **Linear**:使用线性采样器进行上采样，也是默认的方法，也是最通用的方法。使用Linear进行上采样的图像像素之间过渡较为柔和。

3. **Point**:使用点采样器进行上采样。在Inspector窗口中，该模式被命名为"Nearest-Neighbor"，即最邻近采样，使用Point进行上采样的图像像素之间不会做过渡，不建议手动选择此模式进行上采样。

4. **FSR**:其在Inspector窗口中的全称为FidelityFX Super Resolution 1.0，简称FSR，即本文将要详细分析的算法。FSR是AMD开发的一种算法层面的图像超采样技术，对硬件无任何要求，其通过对低分辨率的图像进行重建和重采样来生成高分辨率的图像。

#### 3 URP FSR解读 Analysis of URP FSR

FSR1.0的主要原理部分可参考[《详细剖析 AMD FSR 算法》](https://zhuanlan.zhihu.com/p/401030221)，本文将会结合代码与原理进行一定解读。

FSR的CPU端主要逻辑写在了PostProcessPass.cs的RenderFinalPass(...)函数中。

FSR的主要步骤分为2步：(1)**EASU**，即Edge Adaptive Spatial Upsampling边缘自适应空间**上采样**，可以理解为在普通的上采样基础上，对边缘（锯齿）做特殊的上采样处理。(2)**RCAS**，即Robust Contrast Adaptive Sharpening鲁棒对比度自适应锐化？，可以理解为一次**图像锐化操作**。

##### 3.1 EASU

(1)首先看EASU在**CPU端**的准备工作：

```c#
// EASU
cmd.GetTemporaryRT(ShaderConstants._UpscaledTexture, upscaleRtDesc, FilterMode.Point);
isUpscaledTextureUsed = true;
var fsrInputSize = new Vector2(cameraData.cameraTargetDescriptor.width, cameraData.cameraTargetDescriptor.height);
var fsrOutputSize = new Vector2(cameraData.pixelWidth, cameraData.pixelHeight);
FSRUtils.SetEasuConstants(cmd, fsrInputSize, fsrInputSize, fsrOutputSize);
Blit(cmd, sourceRtId, ShaderConstants._UpscaledTexture, m_Materials.easu);
```

首先**新建一张名为"_UpscaledTexture"的RT**，用于存储EASU上采样的结果，其尺寸取自cameraData.pixelWidth/Height，注意cameraData.pixelWidth/Height即最终RT的尺寸，该数值**未乘以**RenderScale。而应用RenderScale的RT尺寸被存储在cameraData.cameraTargetDescriptor中。

同时，将EASU需要的参数传递给GPU，具体传入的参数如下图所示，Pack成了4个Vector4向量。



![20230614163659](https://raw.githubusercontent.com/recaeee/PicGo/main/recaeee/PicGo20230614163659.png)



使用Blit对输入RT执行EASU操作，将输出RT存入_UpscaledTexture。

(2)接下来看EASU在**GPU端**的工作：

EASU的Vertex Shader部分为简单的FullscreenVert，即后处理常用的Vertex Shader，这里不过多说明。重点说明EASU的Fragment Shader部分。

```hlsl
        half4 FragEASU(Varyings input) : SV_Target
        {
            UNITY_SETUP_STEREO_EYE_INDEX_POST_VERTEX(input);

            float2 uv = UnityStereoTransformScreenSpaceTex(input.uv);
            uint2 integerUv = uv * _ScreenParams.xy;

            half3 color = ApplyEASU(integerUv);

            // Convert to linearly encoded color before we pass our output over to RCAS
#if UNITY_COLORSPACE_GAMMA
            color = GetSRGBToLinear(color);
#else
            color = Gamma20ToLinear(color);
#endif

            return half4(color, 1.0);
        }
```

首先将[0.0f,1.0f]空间下的像素uv转换到[(uint)0,(uint)**outputRTSize**]空间下，即使用整数形式的uv坐标，对应代码如下。

```hlsl
            float2 uv = UnityStereoTransformScreenSpaceTex(input.uv);
            uint2 integerUv = uv * _ScreenParams.xy;//_ScreenParams.xy->最终RT的width & height
```

接下来进入**ApplyEASU**函数，即EASU的算法主体部分。

```hlsl
real3 ApplyEASU(uint2 positionSS)
{
#if FSR_EASU_H
    // Execute 16-bit EASU
    AH3 color;
    FsrEasuH(
#else
    // Execute 32-bit EASU
    AF3 color;
    FsrEasuF(
#endif
        color, positionSS, FSR_EASU_CONSTANTS_0, FSR_EASU_CONSTANTS_1, FSR_EASU_CONSTANTS_2, FSR_EASU_CONSTANTS_3
    );
    return color;
}
```

首先通过**FSR_EASU_H**宏产生了2条分支，其中H的意思为Half，两条分支的区别在于像素RGB值的bit位数不同，激活FSR_EASU_H宏时使用**half类型**的RGB值（即16bit），不激活时使用**float类型**的RGB值（即32bit）。考虑到DX11上的Approximation方法存在一定已知问题，因此**在DX11的API上强制使用Half**，其余则根据平台支持差异使用对应的Float或Half。

在这里以Float类型为例，进入FsrEasuF函数，这里首先注意FsrEasuF函数的传入参数，其中FSR_EASU_CONSTANTS_X即CPU端传入的4个Vector4，但这里通过宏定义**预先将这些float4转换成了uint4**，其内部通过D3DHLSL内部函数asint(x)实现，其作用为将x的位模式解释为整数。这里转换为uint的目的不是很清楚，在进入函数后，又会把这些uint4转换回float4，看起来这步转换uint4没什么意义（或许是防止某些bug或者避免精度丢失？）。

进入FsrEasuF函数的第一步为**计算'坐标F'的uv值**（即代码中的fp）。

```hlsl
 void FsrEasuF(
 out AF3 pix,AU2 ip, AU4 con0, AU4 con1,AU4 con2,AU4 con3){
  // Get position of 'f'.
  AF2 pp=AF2(ip)*AF2_AU2(con0.xy)+AF2_AU2(con0.zw);
  AF2 fp=floor(pp);
  pp-=fp;

  ...
 }
```

什么是'**坐标F**'？在我的理解中，'**坐标F**'是**一个像素f的左上角顶点uv坐标**，**像素f**是inputRT上的一个像素，其次，**对于任意1个outputRT上的像素，其对应的像素f为其覆盖到inputRT的所有像素的左上那个像素**。

更详细点说，考虑到outputRT的纹素一定比inputRT的纹素小，如果将outputRT与inputRT重叠，那根据outputRT的像素位置不同，outputRT的像素覆盖到inputRT的像素数量只可能是3个值：1，2，4。下面如图举例以上这3种情况。如下图所示，对于每个outputRT的像素（橙色），对应一个在inputRT上的**2x2的像素区域**（深色），2x2区域的左上像素即**像素f**，像素f的左上角（蓝色X点）就是**坐标F**。



![20230614155943](https://raw.githubusercontent.com/recaeee/PicGo/main/recaeee/PicGo20230614155943.png)



根据该inputRT上的2x2像素区域（其中左上像素为像素F），将其沿水平、垂直方向各延申1个像素，得到一个“十”字形的像素区域。共12个像素，**这12个像素最后就会对outputRT上的1个像素做出贡献**。在这个12个像素中，首先**对坐标F偏移(1,-1)个纹素距离得到p0**（这里以左上角为uv原点作说明），再根据p0点依次偏移3次得到p1,p2,p3，总共**得到4个采样uv坐标p0、p1、p2、p3**，如下图所示。每次偏移的距离取自CPU端传入的_FsrEasuConstantsX。



![20230614160810](https://raw.githubusercontent.com/recaeee/PicGo/main/recaeee/PicGo20230614160810.png)



对应代码块如下。

```hlsl
  // Allowing dead-code removal to remove the 'z's.
  AF2 p0=fp*AF2_AU2(con1.xy)+AF2_AU2(con1.zw);
  // These are from p0 to avoid pulling two constants on pre-Navi hardware.
  AF2 p1=p0+AF2_AU2(con2.xy);
  AF2 p2=p0+AF2_AU2(con2.zw);
  AF2 p3=p0+AF2_AU2(con3.xy);
```

有一点比较奇怪的是，p0和p3坐标似乎比p1和p2距离中心远1个纹素距离，理论上水平和垂直方向不应该有差异，这里根据代码注释说到对于packed FP16，需要对采样坐标做一定调整，因此为了不分类讨论，统一使用这一套“奇怪”的4个坐标进行Gather4的采样。但这2种方式采样得到的结果就作用来说是一样的，我们只需要得b~o的这12个像素值，通过对p0、p1、p2、p3进行Gather采样，可以得到16个像素值，其中4个不需要用到的像素值舍弃掉就行了。（按正常理论方式采样就会有4次重复的fgjk值）

得到p0、p1、p2、p3四个采样坐标后，**使用TextureGather分别对这4个坐标进行R通道、G通道、B通道采样**，总计4*3=12次采样。

**补充**：TextureGather是一个需要硬件和API支持的采样指令，可以在一个指令中同时取得多个像素的值，从而减少纹理采样的开销。一次TextureGather只能得到纹理中的1个通道的值，并且只能采样Mip0。其返回值为1个Vector4，即左下、右下、右上、左上四个像素对应通道的值。

```hlsl
  AF4 bczzR=FsrEasuRF(p0);
  AF4 bczzG=FsrEasuGF(p0);
  AF4 bczzB=FsrEasuBF(p0);
  AF4 ijfeR=FsrEasuRF(p1);
  AF4 ijfeG=FsrEasuGF(p1);
  AF4 ijfeB=FsrEasuBF(p1);
  AF4 klhgR=FsrEasuRF(p2);
  AF4 klhgG=FsrEasuGF(p2);
  AF4 klhgB=FsrEasuBF(p2);
  AF4 zzonR=FsrEasuRF(p3);
  AF4 zzonG=FsrEasuGF(p3);
  AF4 zzonB=FsrEasuBF(p3);
```

通过4次TextureGather采样，我们得到了b~o共12个像素的RGB值（其中4个无用像素值可以忽略），接下来使用简单的公式**计算各个像素的亮度**（或者叫灰度），其公式满足下式（考虑绿色对亮度贡献最大）。



$Luma=0.5*B+0.5*R+G$



```hlsl
  // Simplest multi-channel approximate luma possible (luma times 2, in 2 FMA/MAD).
  AF4 bczzL=bczzB*AF4_(0.5)+(bczzR*AF4_(0.5)+bczzG);
  AF4 ijfeL=ijfeB*AF4_(0.5)+(ijfeR*AF4_(0.5)+ijfeG);
  AF4 klhgL=klhgB*AF4_(0.5)+(klhgR*AF4_(0.5)+klhgG);
  AF4 zzonL=zzonB*AF4_(0.5)+(zzonR*AF4_(0.5)+zzonG);
```

**重新声明这12个像素的亮度**，从4个Vector4中拆分出12个像素的亮度值，方便后续代码书写。

```hlsl
  // Rename.
  AF1 bL=bczzL.x;
  AF1 cL=bczzL.y;
  AF1 iL=ijfeL.x;
  AF1 jL=ijfeL.y;
  AF1 fL=ijfeL.z;
  AF1 eL=ijfeL.w;
  AF1 kL=klhgL.x;
  AF1 lL=klhgL.y;
  AF1 hL=klhgL.z;
  AF1 gL=klhgL.w;
  AF1 oL=zzonL.z;
  AF1 nL=zzonL.w;
```

为了使上采样的结果更优，需要对最终采样点做一定，因此需要**计算边缘的梯度方向$dir$和边缘特征值$len$**($len$也可以称为$Feature$)。沿着梯度方向像素亮度值变化最快，因此**沿着梯度方向$dir$做采样融合会有更好的效果**（我理解为不会把边缘模糊掉）。同时，我们需要判断本次上采样是否处理的是边缘，如果是边缘，则需要以特殊的权值（高频滤波）去加权像素区域；如果不是边缘，则按常规方法混合（比如双线性采样）就行了。因此我们需要**计算边缘特征值$len$来衡量本次上采样是否处理的是边缘**。

而为了使上采样效果更好，将上采样坐标周围12个像素纳入已知条件来计算梯度方向和边缘特征值。在具体计算中，将12个像素区域分成4个'十'字形区域，对每个'十'字形区域计算梯度方向$dir_x$和和边缘特征值$len_x$，再对4次计算结果做双线性插值。如下图所示，遍历中心2x2的每个像素，以每个像素为中心构成"十"字形像素区域，计算各自的$dir$和$len$，最后得到2x2=4个$dir$和$len$，再**对这4个值以权重$\omega$线性插值**，得到最后的$dir_{final}$和$len_{final}$。(**后文中$x$表示中心像素如f、g、j、k，$X$表示水平方向**)



![20230614182957](https://raw.githubusercontent.com/recaeee/PicGo/main/recaeee/PicGo20230614182957.png)



计算$dir_{final}$和$len_{final}$代码部分如下所示。

```hlsl
// Accumulate for bilinear interpolation.
  AF2 dir=AF2_(0.0);
  AF1 len=AF1_(0.0);
  FsrEasuSetF(dir,len,pp,true, false,false,false,bL,eL,fL,gL,jL);
  FsrEasuSetF(dir,len,pp,false,true ,false,false,cL,fL,gL,hL,kL);
  FsrEasuSetF(dir,len,pp,false,false,true ,false,fL,iL,jL,kL,nL);
  FsrEasuSetF(dir,len,pp,false,false,false,true ,gL,jL,kL,lL,oL);
```

在FsrEasuSetF方法中，首先**计算当前中心像素的$dir$和$len$在最后进行双线性插值时对应的权值$\omega$**，，其计算公式满足下式，其中pp为outputRT中像素在以坐标F为原点下的相对坐标。
其实从公式中也可以看到，是以到"十"字形的最中心为原点（虽然不是用的距离公式，而是以xy棋盘距离相乘的结果）去做双线性插值的。



$\omega_x=
\begin{cases}
(1-pp.x)*(1-pp.y)\quad if\ x\ is\ f\\
pp.x(*1-pp.y)\quad if\ x\ is\ g\\
(1-pp.x)*pp.y\quad if\ x\ is\ j\\
pp.x*pp.y\quad if\ x\ is\ k\\
\end{cases} $



接着计算"十"字形像素区域在水平、垂直方向上的亮度差，并由此**计算亮度变化最快的梯度方向$dir_x$**，其为Vector2类型，以水平方向X为例，其中$dirX$满足下式（这里并不会对梯度方向标准化），**其中$lD$表示像素D的亮度**，像素位置如下图所示。



$dirX_x=(lD-lB)$

$dir_x=(dirX_x,dirY_x)$

![20230614180746](https://raw.githubusercontent.com/recaeee/PicGo/main/recaeee/PicGo20230614180746.png)



在计算梯度方向的同时，计算**边缘特征值$len_x$**，以水平方向$X$为例，其$lenX$满足下式。



$lenX_x=(saturate(\frac{abs(dirX_x)}{max(abs(lD-lC),abs(lC-lB))}))^2$

$len_x=lenX_x+lenY_x$



根据以上公式遍历2x2范围的中心像素，通过**对4个'十'字形的梯度方向$dir_x$和边缘特征值$len_x$按权值$\omega_x$累加**，得到最终的$dir_{final}$和$len_{final}$，如下式所示。



$dir_{final}=\omega_{f}*dir_{f}+\omega_{g}*dir_{g}+\omega_{j}*dir_{j}+\omega_{k}*dir_{k}$



在得到最终的$dir_{final}$和$len_{final}$后，首先**对$dir_{final}$进行标准化**，这里要特殊处理$dirX_{final}=dirY_{final}=0$的情况。对$dir_{final}$标准化后，其实$dir_{final}$就满足$dir_{final}=(cos\theta,sin\theta)$了。

```hlsl
// Normalize with approximation, and cleanup close to zero.
  AF2 dir2=dir*dir;
  AF1 dirR=dir2.x+dir2.y;
  AP1 zro=dirR<AF1_(1.0/32768.0);
  dirR=APrxLoRsqF1(dirR);
  dirR=zro?AF1_(1.0):dirR;
  dir.x=zro?AF1_(1.0):dir.x;
  dir*=AF2_(dirR);
```

接下来将边缘特征值$len_{final}$从[0,2]区间变换到[0,1]区间（[0,2]的原因是因为在前面计算$Len_x$是直接将$LenX_x$和$LenY_x$相加导致的），并且进行平方，如下式所示。由此得到最终的$len_{final}$表示轮廓特征值。



$len_{final}=(\frac{len_{final}}{2})^2$



```hlsl
// Transform from {0 to 2} to {0 to 1} range, and shape with square.
  len=len*AF1_(0.5);
  len*=len;
```

目前我们得到了梯度方向$dir_{final}$和轮廓特征值$len_{final}$。为了减少锯齿，EASU会进一步**根据这两个值计算一个缩放Scale**（代码中命名为len2，其类型为Vector2，即XY方向上的缩放），首先计算$Stretch$，再根据$Stretch$计算$Scale=(Scale_X,Scale_Y)$，计算式如下。

{% raw %}

$Stretch=\frac{{dir_{final}.x}^2+{dir_{final}.y}^2}{max(abs(dir_{final}.x),abs(dir_{final}.y))}$

{% endraw %}

$Scale_X=1+(Stretch-1)*len_{final}$

$Scale_Y=1-0.5*len_{final}$



```hlsl
  // Stretch kernel {1.0 vert|horz, to sqrt(2.0) on diagonal}.
  AF1 stretch=(dir.x*dir.x+dir.y*dir.y)*APrxLoRcpF1(max(abs(dir.x),abs(dir.y)));
  // Anisotropic length after rotation,
  //  x := 1.0 lerp to 'stretch' on edges
  //  y := 1.0 lerp to 2x on edges
  AF2 len2=AF2(AF1_(1.0)+(stretch-AF1_(1.0))*len,AF1_(1.0)+AF1_(-0.5)*len);
```

接下来计算**$lob$(Negative lobe strength)和$clp$**，其计算式如下。$lob$用于最终衡量当前上采样区域的边缘特征（其实就是$len_final$的一个简单映射，最后会应用到近似的lanczos2函数中，lob这个概念应该也属于lanczos函数）。$clp$用于在后续计算自适应权值时设置一个上限阈值。



$lob=0.5+((\frac{1}{4}-0.04)-0.5)*len_{final}$

$clp=\frac{1}{lob}$



接下来计算f、g、j、k4个最近邻像素的RGB最大最小值$max$和$min$（两个Vector3），最终**上采样的计算结果会被Clip到这个范围内**。这里再次给出12个像素的分布图。



![20230616102120](https://raw.githubusercontent.com/recaeee/PicGo/main/recaeee/PicGo20230616102120.png)



```hlsl
AF3 min4=min(AMin3F3(AF3(ijfeR.z,ijfeG.z,ijfeB.z),AF3(klhgR.w,klhgG.w,klhgB.w),AF3(ijfeR.y,ijfeG.y,ijfeB.y)),
               AF3(klhgR.x,klhgG.x,klhgB.x));
AF3 max4=max(AMax3F3(AF3(ijfeR.z,ijfeG.z,ijfeB.z),AF3(klhgR.w,klhgG.w,klhgB.w),AF3(ijfeR.y,ijfeG.y,ijfeB.y)),
               AF3(klhgR.x,klhgG.x,klhgB.x));
```

接下来对12个点的颜色值进行加权。

以b点为例，我们需要**计算b点颜色混合到最终颜色的权值$\omega_x$**，其权值计算公式使用了**近似的lancos2函数**，自变量为**b点到Resolve Position经过自适应后的距离**。那首先计算b点到Resolve Position的自适应距离，将Resolve Position到b点的偏移向量$offset$根据$dir_{final}$进行**旋转**，再根据$Scale$对其进行**缩放**(考虑各向异性)，再计算旋转缩放后$offset$向量的$distance^2$，并将其Clip到$clp$以下。



![20230616114823](https://raw.githubusercontent.com/recaeee/PicGo/main/recaeee/PicGo20230616114823.png)



接下来**根据lanczos2函数的近似计算式求得b点的权值$\omega_x$**，其近似式如下。其中$x$即自适应计算出的$distence^2$，$\omega$为先前计算出的$lob$。在这里也给出原Lanczos的大致函数图式，其相当于**一个高频滤波器**（拉普拉斯算子的一个自适应优化，因为Lanczos函数可根据参数调节，以此可以针对上采样区域调整不同的混合权值），**用于在上采样边缘时对图像进行一定的锐化**。



$\omega_x=[\frac{25}{16}(\frac{2}{5}x^2-1)^2-(\frac{25}{16})-1](\omega x^2-1)^2$

![20230616115519](https://raw.githubusercontent.com/recaeee/PicGo/main/recaeee/PicGo20230616115519.png)



最后根据$\omega_x$加权b点的颜色值。计算自适应权值$\omega_x$与加权颜色的代码块如下。

```hlsl
 void FsrEasuTapF(
 inout AF3 aC, // Accumulated color, with negative lobe.
 inout AF1 aW, // Accumulated weight.
 AF2 off, // Pixel offset from resolve position to tap.
 AF2 dir, // Gradient direction.
 AF2 len, // Length.
 AF1 lob, // Negative lobe strength.
 AF1 clp, // Clipping point.
 AF3 c){ // Tap color.
  // Rotate offset by direction.
  AF2 v;
  v.x=(off.x*( dir.x))+(off.y*dir.y);
  v.y=(off.x*(-dir.y))+(off.y*dir.x);
  // Anisotropy.
  v*=len;
  // Compute distance^2.
  AF1 d2=v.x*v.x+v.y*v.y;
  // Limit to the window as at corner, 2 taps can easily be outside.
  d2=min(d2,clp);
  // Approximation of lancos2 without sin() or rcp(), or sqrt() to get x.
  //  (25/16 * (2/5 * x^2 - 1)^2 - (25/16 - 1)) * (1/4 * x^2 - 1)^2
  //  |_______________________________________|   |_______________|
  //                   base                             window
  // The general form of the 'base' is,
  //  (a*(b*x^2-1)^2-(a-1))
  // Where 'a=1/(2*b-b^2)' and 'b' moves around the negative lobe.
  AF1 wB=AF1_(2.0/5.0)*d2+AF1_(-1.0);
  AF1 wA=lob*d2+AF1_(-1.0);
  wB*=wB;
  wA*=wA;
  wB=AF1_(25.0/16.0)*wB+AF1_(-(25.0/16.0-1.0));
  AF1 w=wB*wA;
  // Do weighted average.
  aC+=c*w;aW+=w;}
  ```

在加权完12个点(b~o)之后，**将最终颜色结果再Clip到先前计算出的$max$和$min$**（即4个最邻近像素的颜色空间内）。由此完成了FSR1.0的**EASU**，即Edge Adaptive Spatial Upsampling边缘自适应空间上采样阶段。

##### 3.2 RCAS

EASU阶段完成后，其实FSR的主要部分已经完成了，接下来的**RCAS阶段**（Robust Contrast Adaptive Sharpening）是对图像进一步地执行**自适应的锐化**。相比EASU，RCAS的逻辑相当简单。

按照惯例，首先看EASU的**CPU部分**：

RCAS阶段的可调参数为**sharpness**，即用于**控制锐化的强度**。在URP管线中可以对该值进行覆写，否则则会使用默认值（默认值为0.92f）。sharpness只有在大于0的时候有意义，其有效最大值为1.0f。

通过锐化强度sharpness，计算出一个sharpnessStops值，其值满足下式。



$sharpnessStops=(1.0-sharpness)*2.5$





接下来根据sharpnessStops计算sharpnessLinear，满足下式。并且将sharpnessLinear降低为half精度（16bits），再将其左移16位，与原本的sharpnessLinear一起pack到一个float值内。相当于用一个float值存了2个half的sharpnessLinear。最后将32位float下的sharpnessLinear和packedSharpnessAsFloat传递给GPU（传递1个名为"_FsrRcasConstants"的Vector4，但其余2个分量为0f）。这里同时传float和half猜测是考虑到供GPU端选择使用哪种精度。



$sharpnessLinear=2^{-sharpnessStops}$



```hlsl
uint sharpnessAsHalf = Mathf.FloatToHalf(sharpnessLinear);
int packedSharpness = (int)(sharpnessAsHalf | (sharpnessAsHalf << 16));
float packedSharpnessAsFloat = BitConverter.Int32BitsToSingle(packedSharpness);
  ```

另外，将EASU上采样得到的RT以"_UpscaledTexture"属性名传递给GPU，同时对Uber.mat激活"_RCAS"的关键字。


接下来到RCAS的**GPU部分**：

RCAS的Shader代码在FinalPost.shader中实现。**RCAS的输入图像必须是Linear颜色空间，如果Unity Settings中使用了Gamma颜色空间，在RCAS之后必须将Linear颜色转换回Gamma空间。**（感觉很少项目会使用Gamma空间？）

在这里以32-bits并且DISABLE_ALPHA的ApplyRCAS函数为例进行解读。

在FsrRcasF函数中，算法使用了最小的**'十'字形邻近像素区域**，其区域示意图如下。



![20230616141341](https://raw.githubusercontent.com/recaeee/PicGo/main/recaeee/PicGo20230616141341.png)



对b、d、e、f、h执行LOAD_TEXTURE2D_X，**共5次采样**，得到这5个像素的RGB值。参考[《这篇文章》](https://gamedev.stackexchange.com/questions/65845/difference-between-texture-load-and-texture-sample-methods-in-directx/65853)，关于LOAD_TEXTURE和SAMPLE_TEXTURE的区别是，**LOAD_TEXTURE只会简单加载某一个特定位置的texel**，并且在纹理范围外的坐标返回0；而SAMPLE_TEXTURE是对纹理图在对应uv位置采样，会应用纹理寻址模式（Clamp、Repeat等）和纹理过滤模式（Bilinear等）。

**计算这5个像素的亮度**，亮度计算公式依然如下。



$luma=0.5*B+0.5*R+G$



**计算噪声值$nz$**，进行噪声检测，这里可以看到首先计算了中心像素与周围4个像素的亮度差先平均再取绝对值（这里会考虑亮度差的方向），并除以最大亮度差。举个例子来简单理解这个公式的意义，假如中心亮度为0，周围4个像素亮度均为1，则$nz=-0.5*1+1.0=0.5$。再假如中心亮度为0，周围2个像素亮度为1，两个像素亮度为0，则$nz=-0.5*0.5+1.0=0.75$。



$nz=-0.5*(satureate(abs(0.25*bL+0.25*dL+0.25*fL+0.25*hL-eL))*\frac{1}{maxLuma-minLuma})+1.0$



接下来**计算RCAS算子周围4个像素的权值$\omega$**。首先计算周围4个像素b、d、f、h的RGB各通道的最大值、最小值。接下来计算各通道的$\omega$值（代码中命名为lobe），以R通道为例，$\omega_R$的计算满足下式，其中$min4p_R$表示周围4个像素R通道最小值，$max4p_R$表示周围4个像素R通道最大值，$eR$表示中心像素R通道值



$\omega_R=max(-\frac{min(min4p_R,eR)}{4*max4p_R},\frac{1-max(max4p_R,eR)}{4*min4p_R-4})$



计算完$\omega_R$、$\omega_G$、$\omega_B$之后，**计算最终的权值$\omega$**。其计算满足下式。



$\omega=max(0.1875,min(max(\omega_R,\omega_G,\omega_B),0.0))*sharpnessLinear$



如果开启噪声抑制，则将$\omega$乘以系数$nz$，如果中心像素越可能是噪声，则周围4个像素的权重越大（但在hlsl代码中，似乎永远不会开启噪声抑制，那这样上面那段$nz$的计算是完全多余的？）。

最后**进行锐化的颜色混合操作**，以R通道为例，其加权满足下计算式。



$pix_R=\frac{\omega(bR+dR+hR+fR)+eR}{4\omega+1}$



由此，RCAS阶段结束，FSR1.0完成。

#### 参考
1. https://gpuopen.com/fidelityfx-superresolution/
2. https://zhuanlan.zhihu.com/p/401030221
3. https://learn.microsoft.com/zh-cn/windows/win32/direct3dhlsl/asint
4. https://learn.microsoft.com/en-us/windows/win32/direct3dhlsl/dx-graphics-hlsl-to-gather
5. https://www.zhihu.com/column/p/31873386
6. https://michaldrobot.files.wordpress.com/2014/05/gcn_alu_opt_digitaldragons2014.pdf
7. https://github.com/GPUOpen-Effects/FidelityFX-FSR
8. https://en.wikipedia.org/wiki/Lanczos_resampling
9. https://gamedev.stackexchange.com/questions/65845/difference-between-texture-load-and-texture-sample-methods-in-directx/65853
10. https://blog.csdn.net/weixin_45979158/article/details/109172985
11. 题图来自画师wlop

